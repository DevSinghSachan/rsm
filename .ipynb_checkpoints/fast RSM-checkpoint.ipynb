{
 "metadata": {
  "name": "",
  "signature": "sha256:f359ac4dd377d3e6b2586594052ba73406add84977721fd135c30c8cc6c66419"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rsm dependencies\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "import numpy\n",
      "from math import floor\n",
      "from imp import reload"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# dataset dependencies\n",
      "from batch_data import BatchData as Batch\n",
      "import utils"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We make sure Mongo is running somewhere :\n",
      "# type mongod somewhere\n",
      "utils.connect_to_database(database_name = 'yelp')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 'restaurants' is the name of the collection, we stem the words in the triggers,\n",
      "# and we lowercase them to minimize the visible dimensions (bag of words dimensions)\n",
      "lexicon = utils.gather_lexicon('restaurants',\n",
      "                               stem= True, \n",
      "                               lowercase = True,\n",
      "                               show_progress= True)\n",
      "lexicon.save(\"lexicon.gzp\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# or we can load a lexicon from the disk:\n",
      "lexicon = utils.Lexicon.load(\"lexicon.gzp\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc = utils.ResourceConverter(lexicon = lexicon)\n",
      "big_batch_size = 1000\n",
      "batch = Batch(\n",
      "    data=utils.mongo_database_global['restaurants'].find({}, {'signature':1}), # from Mongo's cursor enumerator\n",
      "    batch_size = big_batch_size,  # mini-batch\n",
      "    shuffle = True, # stochastic\n",
      "    conversion = rc.process # convert to matrices using lexicon)\n",
      ")\n",
      "train_set_x_mem = batch.next()\n",
      "numpy.save(\"training_set\", train_set_x_mem)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# or we can load a training set from the disk\n",
      "train_set_x_mem = numpy.load(\"training_set.npy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# require theano (GPU deps starts stealing memory from GPU as soon as its loaded)\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from fast_rsm import RSM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_x = theano.shared(train_set_x_mem, borrow = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# construct the RSM class\n",
      "\n",
      "mini_batch_size = 100\n",
      "\n",
      "# allocate symbolic variables for the data\n",
      "\n",
      "n_train_batches = floor(train_set_x.get_value(borrow=True).shape[0] / mini_batch_size)\n",
      "\n",
      "rng = numpy.random.RandomState(123)\n",
      "theano_rng = T.shared_randomstreams.RandomStreams(rng.randint(2 ** 30))\n",
      "n_hidden = 200\n",
      "\n",
      "rsm = RSM(n_visible=lexicon.max_index, n_hidden=n_hidden, numpy_rng=rng, theano_rng=theano_rng)\n",
      "\n",
      "# \t# get the cost and the gradient corresponding to one step of CD-15\n",
      "# \tcost, updates = rbm.get_cost_updates(lr=learning_rate,\n",
      "# \t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get training function\n",
      "learning_rate  = 0.01\n",
      "cost, updates = rsm.get_cost_updates(lr=learning_rate, k=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pycuda.driver\n",
      "# free / total\n",
      "print(\"GPU memory %1f MB free, %1f MB total\" % tuple([i / (1024 ** 2) for i in pycuda.driver.mem_get_info()]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GPU memory 57.125000 MB free, 1023.687500 MB total\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = T.lscalar()    # index to a [mini]batch\n",
      "train_rbm = theano.function(\n",
      "    [index],\n",
      "    cost,\n",
      "    updates=updates,\n",
      "    givens={\n",
      "        rsm.input: train_set_x[index * mini_batch_size:(index + 1) * mini_batch_size],\n",
      "        rsm.scaling: train_set_x[index * mini_batch_size:(index + 1) * mini_batch_size].sum(axis=1).astype(theano.config.floatX),\n",
      "    },\n",
      "    name='train_rbm')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_epochs = 15\n",
      "for epoch in range(training_epochs):fc\n",
      "    # go through the training set\n",
      "    mean_cost = []\n",
      "    for batch_index in range(n_train_batches):\n",
      "        # free / total\n",
      "        mean_cost.append(train_rbm(batch_index))\n",
      "        print('Training batch %d of epoch %d, cost is %.2f' % (batch_index, epoch, numpy.mean(mean_cost)))\n",
      "    print('Training epoch %d, cost is %.2f' % (epoch, numpy.mean(mean_cost)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training epoch 0, cost is  -5305.44\n",
        "Training epoch 1, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4137.41\n",
        "Training epoch 2, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4073.38\n",
        "Training epoch 3, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -3980.91\n",
        "Training epoch 4, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4003.43\n",
        "Training epoch 5, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4123.35\n",
        "Training epoch 6, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4309.89\n",
        "Training epoch 7, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4405.09\n",
        "Training epoch 8, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4743.58\n",
        "Training epoch 9, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -5311.57\n",
        "Training epoch 10, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -4448.06\n",
        "Training epoch 11, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -5334.42\n",
        "Training epoch 12, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -7151.01\n",
        "Training epoch 13, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -7747.42\n",
        "Training epoch 14, cost is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " -7974.82\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}