{
 "metadata": {
  "name": "",
  "signature": "sha256:1aae00073815fbcec09acef533ca52dffabec7b8e997b1038d09b0933c001943"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# rsm dependencies\n",
      "%load_ext autoreload\n",
      "%autoreload 2\n",
      "import numpy\n",
      "from math import floor\n",
      "from imp import reload\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# dataset dependencies\n",
      "from batch_data import BatchData as Batch\n",
      "import utils"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We make sure Mongo is running somewhere :\n",
      "# type mongod somewhere\n",
      "utils.connect_to_database(database_name = 'yelp')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 'restaurants' is the name of the collection, we stem the words in the triggers,\n",
      "# and we lowercase them to minimize the visible dimensions (bag of words dimensions)\n",
      "lexicon = utils.gather_lexicon('restaurants',\n",
      "                               stem= True, \n",
      "                               lowercase = True,\n",
      "                               show_progress= True)\n",
      "lexicon.save(\"lexicon.gzp\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# or we can load a lexicon from the disk:\n",
      "lexicon = utils.Lexicon.load(\"lexicon.gzp\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc = utils.ResourceConverter(lexicon = lexicon)\n",
      "big_batch_size = 1000\n",
      "batch = Batch(\n",
      "    data=utils.mongo_database_global['restaurants'].find({}, {'signature':1}), # from Mongo's cursor enumerator\n",
      "    batch_size = big_batch_size,  # mini-batch\n",
      "    shuffle = True, # stochastic\n",
      "    conversion = rc.process # convert to matrices using lexicon)\n",
      ")\n",
      "train_set_x_mem = batch.next()\n",
      "numpy.save(\"training_set\", train_set_x_mem)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# or we can load a training set from the disk\n",
      "train_set_x_mem = numpy.load(\"training_set.npy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# require theano (GPU deps starts stealing memory from GPU as soon as its loaded)\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "from fast_rsm import RSM"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set_x = theano.shared(train_set_x_mem, borrow = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# construct the RSM class\n",
      "\n",
      "mini_batch_size = 100\n",
      "\n",
      "# allocate symbolic variables for the data\n",
      "\n",
      "n_train_batches = floor(train_set_x.get_value(borrow=True).shape[0] / mini_batch_size)\n",
      "\n",
      "rng = numpy.random.RandomState(123)\n",
      "theano_rng = T.shared_randomstreams.RandomStreams(rng.randint(2 ** 30))\n",
      "n_hidden = 200\n",
      "\n",
      "rsm = RSM(n_visible=lexicon.max_index, n_hidden=n_hidden, numpy_rng=rng, theano_rng=theano_rng)\n",
      "\n",
      "# \t# get the cost and the gradient corresponding to one step of CD-15\n",
      "# \tcost, updates = rbm.get_cost_updates(lr=learning_rate,\n",
      "# \t"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get training function\n",
      "learning_rate  = 0.01\n",
      "cost, updates = rsm.get_cost_updates(lr=learning_rate, k=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pycuda.driver\n",
      "# free / total\n",
      "print(\"GPU memory %1f MB free, %1f MB total\" % tuple([i / (1024 ** 2) for i in pycuda.driver.mem_get_info()]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index = T.lscalar()    # index to a [mini]batch\n",
      "train_rbm = theano.function(\n",
      "    [index],\n",
      "    cost,\n",
      "    updates=updates,\n",
      "    givens={\n",
      "        rsm.input: train_set_x[index * mini_batch_size:(index + 1) * mini_batch_size],\n",
      "        rsm.scaling: train_set_x[index * mini_batch_size:(index + 1) * mini_batch_size].sum(axis=1).astype(theano.config.floatX),\n",
      "    },\n",
      "    name='train_rbm')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "training_epochs = 50\n",
      "errors          = []\n",
      "training_indices = [i for i in range(n_train_batches)]\n",
      "for epoch in range(training_epochs):\n",
      "    # go through the training set\n",
      "    mean_cost = []\n",
      "    random.shuffle(training_indices)\n",
      "    for batch_index in training_indices:\n",
      "        # free / total\n",
      "        mean_cost.append(train_rbm(batch_index))\n",
      "        print('.')\n",
      "    print('Training epoch %d, cost is %.2f' % (epoch, numpy.mean(mean_cost)))\n",
      "    errors.append(numpy.mean(mean_cost))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML, Latex, Display\n",
      "def table_styling():\n",
      "    return HTML(\"\"\"\n",
      "    <style>\n",
      "    table.python_table {\n",
      "         margin: 0          !important;\n",
      "        padding: 0          !important;\n",
      "    }\n",
      "    table.python_table td, table.python_table th  {\n",
      "        border-spacing: 0   !important;\n",
      "        vertical-align: top !important;\n",
      "        padding: 5px;\n",
      "        border: 1px solid #ccc;\n",
      "        font-family: \"Lucida Grande\";\n",
      "    }\n",
      "    table.python_table th {\n",
      "        text-align: center;\n",
      "    }\n",
      "    \n",
      "    table.python_table td > table td, table.python_table td > table tr, table.python_table td > table th {\n",
      "        border: 1px solid #ddd;\n",
      "    }\n",
      "    \n",
      "    </style>\n",
      "    \n",
      "    \"\"\")\n",
      "def to_table(datum, **kwargs):\n",
      "    title = ''\n",
      "    colspan = str(len(datum)) if kwargs.get('axis') == 1 else '1'\n",
      "    if kwargs.get('title'):\n",
      "        title = '<tr><th colspan='+colspan+'>'+kwargs.get('title')+'</th></tr>'\n",
      "    if kwargs.get('axis') == 1:\n",
      "        cols = [\"<td>\"+ i +\"</td>\" for i in datum]\n",
      "        return \"<table class='python_table' cellpadding='0'><thead>\"+title+\"</thead><tbody><tr>\" + \"\".join(cols) + \"</tr></tbody></table>\"\n",
      "    else:\n",
      "        rows = [\"<tr><td>\"+ i +\"</td></tr>\" for i in datum]\n",
      "        return \"<table class='python_table' cellpadding='0'><thead>\"+title+\"</thead><tbody>\" + \"\".join(rows) + \"</tbody></table>\"\n",
      "def show_in_table(datum, **kwargs):\n",
      "    return HTML(to_table(datum, **kwargs))\n",
      "table_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reconstruction_fn():\n",
      "    sample = theano.tensor.matrix('sample')\n",
      "    reconstruction = rsm.gibbs_vhv(sample)\n",
      "    return theano.function([sample], reconstruction, givens= {\n",
      "        rsm.input : sample,\n",
      "        rsm.scaling: sample.sum(axis=1).astype(theano.config.floatX)})\n",
      "reconstruct = reconstruction_fn()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words_counts = train_set_x_mem.sum(axis=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "restaurant     = train_set_x_mem\n",
      "reconstruction = set([lexicon.reverse_lexicon[i]+(' (%d)' % words_counts[i]) for i in np.nonzero(reconstruct(restaurant)[3])[0]])\n",
      "original       = set([lexicon.reverse_lexicon[i]+(' (%d)' % words_counts[i]) for i in np.nonzero(restaurant[3])[0]])\n",
      "imagination    = reconstruction - original\n",
      "oubli          = original - reconstruction\n",
      "memory         = original - oubli\n",
      "show_in_table(\n",
      "    [\n",
      "        to_table(original, title= 'Original'),\n",
      "        to_table(memory, title= 'Memory'),\n",
      "        to_table(oubli, title= 'Oubli'),\n",
      "        to_table(reconstruction, title= 'Reconstruction'),\n",
      "        to_table(imagination, title='Imagination')\n",
      "    ],\n",
      "    axis=1, title='RSM results')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filter_position = 100\n",
      "words_to_show = 10\n",
      "one_hot = np.zeros([1,200])\n",
      "one_hot[0,filter_position] = 1.\n",
      "pre_sigmoid = np.dot(one_hot, rsm.W.get_value().T) + rsm.vbias.get_value()\n",
      "tmp = np.exp(pre_sigmoid)\n",
      "sum = tmp.sum(axis=1)\n",
      "pdf = tmp / sum\n",
      "words_in_creation = np.random.multinomial(words_to_show, pdf[0,:], size=1)\n",
      "creation = set([lexicon.reverse_lexicon[i]+(' (%d)' % words_counts[i]) for i in np.nonzero(words_in_creation)[1]])\n",
      "show_in_table(\n",
      "    [\n",
      "        to_table(creation, title= ('Filter %d' % filter_position), axis=1),\n",
      "    ]\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hidden_projection_fn():\n",
      "    sample = theano.tensor.matrix('sample')\n",
      "    [pre_sigmoid_h1, h1_mean, h1_sample] = rsm.sample_h_given_v(sample)\n",
      "    return theano.function([sample], h1_sample, givens = {rsm.scaling: sample.sum(axis=1)})\n",
      "hidden_projection = hidden_projection_fn()\n",
      "def propup_fn():\n",
      "    sample = theano.tensor.matrix('sample')\n",
      "    pre_sigmoid_h1, h1_mean = rsm.propup(sample)\n",
      "    return theano.function([sample], (pre_sigmoid_h1, h1_mean), givens = {rsm.scaling: sample.sum(axis=1)})\n",
      "hidden_projection = hidden_projection_fn()\n",
      "propup = propup_fn()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Let us compare the gradient approach, and the dot product approach"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*They are identical:*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v_sample    = T.matrix('v_sample')\n",
      "scaling     = v_sample.sum(axis=1)\n",
      "W           = T.matrix('W')\n",
      "hbias       = T.vector('hbias')\n",
      "vbias       = T.vector('vbias')\n",
      "wx_b        = T.dot(v_sample, W) + T.outer(scaling, hbias)\n",
      "vbias_term  = T.dot(v_sample, vbias)\n",
      "hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
      "gradient = T.grad(T.mean(-hidden_term - vbias_term), W)\n",
      "alternative = - T.dot(v_sample.T, T.nnet.sigmoid(wx_b)) / v_sample.shape[0]\n",
      "\n",
      "compare_func= theano.function([v_sample, W, hbias, vbias], alternative - gradient, on_unused_input='ignore')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_visible = 5\n",
      "n_hidden  = 2\n",
      "n_examples = 100\n",
      "vals = [\n",
      "    numpy.random.binomial(1, 0.5, (n_examples,n_visible)).astype('float32'),\n",
      "    numpy.random.randn(n_visible, n_hidden).astype('float32'),\n",
      "    0.05 * numpy.random.randn(n_hidden).astype('float32'),\n",
      "    0.05 * numpy.random.randn(n_visible).astype('float32')\n",
      "]\n",
      "\n",
      "numpy.linalg.norm(compare_func(*vals)) < 1e-6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run fast_rsm.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training epoch 0, cost is 5.92, validation cost is 6.12\n",
        "Training epoch 1, cost is 5.58, validation cost is 5.98"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 2, cost is 5.48, validation cost is 5.89"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 3, cost is 5.44, validation cost is 5.88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 4, cost is 5.41, validation cost is 5.88"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 5, cost is 5.40, validation cost is 5.84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 6, cost is 5.38, validation cost is 5.84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 7, cost is 5.37, validation cost is 5.85"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 8, cost is 5.36, validation cost is 5.83"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 9, cost is 5.35, validation cost is 5.80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 10, cost is 5.36, validation cost is 5.83"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 11, cost is 5.36, validation cost is 5.81"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 12, cost is 5.37, validation cost is 5.80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 13, cost is 5.36, validation cost is 5.84"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training epoch 14, cost is 5.35, validation cost is 5.80"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}