{
 "metadata": {
  "name": "",
  "signature": "sha256:486f32baf18e77289d2b1c1bf2fb061c8f7f31af82ffedff36328655812920b7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from imp import reload\n",
      "import rsm\n",
      "import numpy as np\n",
      "import theano as T\n",
      "from matplotlib import pyplot as plt\n",
      "import utils\n",
      "import batch_data\n",
      "from batch_data import BatchData as Batch\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if you need to reload the code shift-enter here !\n",
      "reload(utils)\n",
      "reload(batch_data)\n",
      "from batch_data import BatchData as Batch"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Connect to **MongoDB**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make sure mongo is running somewhere :\n",
      "utils.connect_to_database(database_name = 'yelp')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use database to construct a lexicon (hash-table mapping words to vector dimensions):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lexicon, reverse_lexicon = utils.gather_lexicon('restaurants')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a **Replicated Softmax Machine**:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# if you need to reload the replicated softmax code:\n",
      "reload(rsm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "<module 'rsm' from './rsm.py'>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "batch_size = 100\n",
      "learning_rate = 0.001 / batch_size\n",
      "encoder = rsm.RSM(momentum = 0.9, data = np.zeros([0,len(lexicon.items())]), hidden_units = 123, learning_rate=learning_rate)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create the stochastic batch element with 100 elements per mini-batch:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rc = utils.ResourceConverter(lexicon = lexicon)\n",
      "batch = Batch(\n",
      "    data=utils.mongo_database_global['restaurants'].find(), # from Mongo's cursor enumerator\n",
      "    batch_size =batch_size,  # mini-batch\n",
      "    shuffle = True, # stochastic\n",
      "    conversion = rc.process # convert to matrices using lexicon)\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start mini-batch learning for 1000 epochs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochs = 1\n",
      "errors = np.zeros(epochs)\n",
      "\n",
      "for epoch in range(epochs):\n",
      "    if epoch > 0 and epoch % 50 == 0:\n",
      "        encoder.k = int(1 + (float(epoch) / float(epochs)) * 4)\n",
      "        encoder.learning_rate = encoder.learning_rate.get_value()\n",
      "        utils.plot_progress(errors[90:], ylabel=\"Perplexity\")\n",
      "    encoder.data = batch.next()\n",
      "    errors[epoch] = encoder.train()\n",
      "    print(\"Epoch[%2d] : Perplexity = %.02f [# Gibbs steps=%d]\" % (epoch, errors[epoch],encoder.k))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Epoch[ 0] : Perplexity = 35735.65 [# Gibbs steps=1]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "encoder.project_into_hidden_layer(batch.next())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "(array([[ 0.65465474,  0.64157814,  0.64434409, ...,  0.62495887,\n",
        "          0.65556067,  0.65649199],\n",
        "        [ 0.73459256,  0.70833492,  0.7217043 , ...,  0.68676484,\n",
        "          0.7379483 ,  0.72651994],\n",
        "        [ 0.7526387 ,  0.72580737,  0.73694324, ...,  0.71644318,\n",
        "          0.75406379,  0.74324286],\n",
        "        ..., \n",
        "        [ 0.80033165,  0.78343683,  0.78373009, ...,  0.78806895,\n",
        "          0.81044453,  0.81087863],\n",
        "        [ 0.66980195,  0.65142417,  0.68518102, ...,  0.67130905,\n",
        "          0.68458307,  0.66211557],\n",
        "        [ 0.76442569,  0.72187692,  0.72847354, ...,  0.73571235,\n",
        "          0.77235222,  0.76382923]], dtype=float32),\n",
        " array([ 452.,  597.,  783.,  784.,  950.,  619.,  455.,  588.,  410.,\n",
        "         844.,  887.,  494.,  707.,  600.,  653.,  638.,  484.,  379.,\n",
        "         608.,  606.,  603.,  596.,  553.,  726.,  844.,  613.,  525.,\n",
        "         579.,  703.,  627.,  669.,  634.,  700.,  604.,  779.,  727.,\n",
        "         679.,  693.,  713.,  657.,  883.,  943.,  755.,  723.,  628.,\n",
        "         696.,  914.,  379.,  672.,  776.,  709.,  608.,  397.,  569.,\n",
        "         436.,  824.,  643.,  738.,  689.,  501.,  702.,  923.,  463.,\n",
        "         928.,  659.,  671.,  442.,  859.,  448.,  636.,  448.,  745.,\n",
        "         539.,  472.,  759.,  558.,  522.,  575.,  653.,  658.,  740.,\n",
        "         631.,  549.,  591.,  549.,  729.,  686.,  718.,  558.,  436.,\n",
        "         565.,  816.,  569.,  448.,  699.,  588.,  624.,  855.,  808.,  528.], dtype=float32))"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.0"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}